AWS S3 and KMS Essentials Guide
Here is a study guide based on the information contained in the sources, covering important topics and notes for each, with sample examples understandable by someone new to AWS Cloud.
AWS S3 and KMS Study Guide
This guide summarizes key concepts related to Amazon S3 and AWS Key Management Service (KMS) as presented in the provided sources.
•
S3 Security (General)
◦
Note: By default, S3 is private [1]. The only identity with initial access to an S3 bucket is the account root user of the account that owns the bucket [1]. Any other permissions must be explicitly granted [1].
◦
Example: When you first create a new S3 bucket, imagine it's like a locked box in the cloud. Only the main administrator account that created the box can open it initially [1]. If you want anyone else, even someone in your own company's AWS account or on the public internet, to access anything inside, you have to specifically give them permission [1].
•
S3 Bucket Policies
◦
Note: S3 bucket policies are a type of AWS resource policy attached to an S3 bucket [1]. They control who can access the resource (the bucket and its objects) [1]. They can grant access to identities within the same account, identities in different AWS accounts, or even anonymous principals (the world) [1, 2]. Bucket policies are the preferred method for granting anonymous access or access to other AWS accounts [3]. They are very flexible and can include conditions, such as blocking specific IP addresses or requiring MFA [3-5]. There can be only one bucket policy on a bucket, but it can contain multiple statements [4].
◦
Example: Imagine you have a bucket named "secret-cat-project" [2]. You can add a bucket policy to this bucket that says: "Allow anyone (any principal, specified by a '*') to read any object (action s3:GetObject) in this bucket" [3]. This makes the objects publicly readable [3]. Another example could be a policy that says: "Deny access (effect deny) to anyone (any principal) trying to get objects (action s3:GetObject) from this bucket unless their IP address (condition SourceIp) is specifically 1.3.3.7" [3].
•
S3 Identity Policies
◦
Note: Identity policies are attached to identities (like users, groups, or roles) [1]. They control what that identity can access [1]. They generally don't have an explicit 'principal' component because it's implied that the identity the policy is attached to is the principal [2, 3]. Identity policies can only control security inside your account; they cannot grant an identity in another account access to resources in your account [1, 6].
◦
Example: You have an AWS user named "ReportReader". You attach an identity policy to this user that says: "Allow this user to read files (action s3:GetObject) from a specific bucket called 'monthly-reports-data'" [Based on 1]. This policy defines what "ReportReader" can do [1].
•
S3 ACLs (Access Control Lists)
◦
Note: ACLs are a legacy form of S3 security, applied to objects or buckets [5]. AWS recommends using bucket policies or identity policies instead because ACLs are inflexible and only allow very simple permissions [5]. They cannot include conditions [5]. There are only five permissions available in an ACL: Read, Write, Read ACP, Write ACP, and Full Control [5]. You can configure ACLs on a bucket or an object, but not on a group of objects (prefix) [5]. AWS may stop using ACLs at some point [5, 7].
◦
Example: You could use a legacy ACL on a specific object to allow everyone (identified by a special grantee) to "Read" that single object [Based on 5]. However, you couldn't use an ACL to say "Allow only users from this IP address to read this folder", because ACLs lack conditions and cannot target folders (prefixes) [5].
•
S3 Block Public Access
◦
Note: This feature was added to prevent accidental data leaks from incorrectly configured public buckets [8]. It provides an extra layer of security [8]. Block Public Access settings apply no matter what bucket policies or ACLs say, but they only apply to public access (access by anonymous principals or through ACLs) [8]. They do not affect access granted to defined AWS identities [8]. These settings can block any public access entirely (a full override/failsafe) or block only new public access grants while allowing existing ones [6, 8].
◦
Example: You might set a bucket policy to allow public read access to a bucket [3]. However, if the bucket's Block Public Access settings are configured to "Block all public access", then no one can access the objects publicly, even though the bucket policy allows it [8]. This acts as a safety switch [6, 8].
•
S3 Static Website Hosting
◦
Note: This feature allows access to S3 objects via standard HTTP using a web browser, instead of just the AWS APIs [9]. It's useful for hosting static websites like blogs or offloading static media (like images) from compute services [10]. To enable it, you must set an index document (the default page, e.g., index.html) and an error document (shown when something goes wrong, e.g., error.html) [9, 11]. Both should be HTML documents [9]. Enabling this feature creates a static website hosting endpoint URL, which is influenced by the bucket name and region [9, 12]. You can use a custom domain name, but the bucket name must match the custom domain name for this to work directly [10, 13, 14]. Enabling static website hosting itself does not grant public access; you still need a bucket policy to allow public s3:GetObject permissions [15, 16].
◦
Example: You can host a simple blog by putting your HTML and image files in an S3 bucket and enabling static website hosting [17, 18]. You'd configure index.html as the index document [11]. When someone browses to the bucket's website endpoint, S3 will automatically serve the index.html file [12]. If they try to access a file that doesn't exist, the configured error document will be served [11, 12].
•
S3 Pricing (Components)
◦
Note: S3 pricing has three main components: storage costs (per gigabyte per month), data transfer fees (per gigabyte transferred out of S3, transfer into S3 is always free), and request charges (per 1,000 requests/operations like Get, List, Put) [17, 19]. Storage and data transfer are generally very cheap [17]. Request charges can become significant if using features that involve many operations, like static website hosting with a large user base [17].
◦
Example: Storing a 1GB file for a month in S3 Standard costs a certain amount per GB/month [19]. Downloading that file multiple times adds data transfer fees (for transferring data out) and request fees (for each download operation) [17, 19]. Uploading the file initially costs nothing for data transfer, but there's a request charge for the upload operation [17, 19].
•
S3 Object Versioning
◦
Note: Object versioning is controlled at the bucket level and is disabled by default [20]. Once enabled, it cannot be disabled, only suspended [20, 21]. Enabling versioning means that instead of overwriting or deleting an object when it's modified, S3 keeps the original version and creates a new version [20, 22]. Each version is assigned a unique version ID [22]. Accessing an object without specifying a version ID returns the current (newest) version [20, 23]. Deleting an object without specifying a version ID doesn't permanently delete it; it adds a delete marker, which becomes the current version and hides previous versions [20, 24]. Deleting a delete marker "undeletes" the object, making the previous version current again [20, 24]. Permanently deleting a specific version requires specifying its version ID [20, 24]. Versioning can increase storage costs because all versions are stored [20, 21]. Suspending versioning stops new versions from being created but does not remove existing ones [20, 21].
◦
Example: You upload a picture of your cat, Winky, named winky.jpeg, to a bucket with versioning enabled [20]. This becomes version ID 1111 [20]. Later, you accidentally upload a picture of a dog, also named winky.jpeg [20]. S3 doesn't replace the cat picture; it keeps it and creates a new version (version ID 2222) for the dog picture. Version 2222 is the current version [20]. If you download winky.jpeg normally, you get the dog picture [23]. But you can request version 1111 specifically to get the original cat picture back [20, 21, 24]. If you delete winky.jpeg without specifying a version, a delete marker is added, and it looks like the object is gone, but the old versions are still there [20, 24].
•
S3 MFA Delete
◦
Note: MFA Delete is an optional feature enabled within the versioning configuration on a bucket [20]. When enabled, it requires MFA (Multi-Factor Authentication) to change the bucket's versioning state (enabled to suspended or vice versa) or to permanently delete any object versions [20]. This adds an extra layer of protection against accidental or malicious deletion of data in versioned buckets [20].
◦
Example: To help prevent an administrator from accidentally or maliciously deleting critical versions of files, you can enable MFA Delete on the bucket [20]. If an administrator later tries to permanently delete a specific version of an object, they will not only need the correct AWS permissions but also a valid MFA token code to complete the operation [20].
•
S3 Performance Optimization (Multi-part Upload)
◦
Note: By default, objects are uploaded as a single stream using the PutObject API call [25]. This is unreliable for large files, as a failure requires restarting the entire upload [25]. Multi-part upload breaks data into smaller parts, each uploaded in isolation [26]. This significantly improves reliability (individual parts can be retried) and speed (parts can be uploaded in parallel) [26, 27]. The minimum size for using multi-part upload is 100MB [26]. It is recommended for any upload over 100MB [26].
◦
Example: A remote worker needs to upload a 2GB video file over an unreliable internet connection [25]. Using a single upload stream might fail halfway through, requiring a full restart, wasting time and bandwidth [25]. With multi-part upload, the file is split into many smaller parts (e.g., 100MB each) [26]. If one part fails to upload, only that small part needs to be retried, not the entire 2GB file [27]. Also, multiple parts can upload simultaneously, utilizing more of the available internet speed [27].
•
S3 Performance Optimization (Transfer Acceleration)
◦
Note: Transfer acceleration speeds up data transfer to and from S3 buckets over long distances [28]. It uses AWS's global network of Edge Locations [28]. Instead of data traveling across the public internet directly to the S3 bucket's region, it first goes to the nearest Edge Location over the public internet (a short, high-performing hop) [28]. From the Edge Location, data travels over the AWS global network, which is optimized for speed and lower latency, to the destination region [28, 29]. Enabling this feature provides a specific endpoint for the bucket that must be used to benefit from acceleration [30]. The benefits are more significant over larger distances [29]. Bucket names for transfer acceleration cannot contain periods [28, 30].
◦
Example: An employee in Australia needs to upload large files to an S3 bucket located in the UK [27, 28]. Normally, the data would travel across the potentially less efficient public internet for the entire distance [28]. With transfer acceleration enabled on the bucket, the data first uploads quickly to an AWS Edge Location near the employee in Australia. From there, it travels over AWS's optimized global network to the UK S3 bucket [28, 29]. This usually results in faster and more reliable uploads compared to sending the data directly over the public internet [29, 30].
•
AWS KMS (Key Management Service) Fundamentals
◦
Note: KMS is a regional, public service used to create, store, and manage cryptographic keys [31]. Keys never leave the KMS product; cryptographic operations (like encrypt/decrypt) are performed within KMS using the keys [31-34]. KMS is FIPS 140-2 level 2 compliant [32]. It handles both symmetric and asymmetric keys, although symmetric keys are often the focus in introductory material [31, 32].
◦
Example: Think of KMS as a secure vault for your digital keys [31]. You can ask the vault to create a new key for you. If you need to encrypt something, you send the data to the vault and tell it which key to use; the vault encrypts the data and sends it back, but the key itself stays safely inside the vault [33]. The same process happens for decryption [33].
•
KMS Keys (KMS keys / CMKs)
◦
Note: These are the main type of key KMS manages, sometimes called Customer Master Keys (CMKs) [32]. A KMS key is a logical container for the actual physical key material used in cryptographic operations [32]. This physical key material can be generated by KMS or imported [32, 35]. KMS keys can directly encrypt or decrypt data up to 4 KB in size [32, 36]. They contain attributes like a key ID, creation date, key policy, description, and state [32].
◦
Example: You create a KMS key named "MyAppDataKey" [Based on 47]. This key has a unique ID and contains the actual secret code (key material) that does the encrypting and decrypting [32]. While the "MyAppDataKey" identifier is visible, the actual secret code inside is protected by KMS and never leaves [32].
•
KMS Data Encryption Keys (DEKs)
◦
Note: DEKs are generated by KMS using a KMS key [36]. They are used to encrypt and decrypt data larger than 4 KB [36]. KMS provides two versions of a DEK: a plain text version (for immediate use by you or a service) and a cipher text/encrypted version (encrypted by the KMS key that generated it) [36]. KMS does not store the DEK itself after generating and providing it; it discards it [36]. The actual encryption/decryption of data larger than 4 KB using a DEK is done by you or the service using KMS, not by KMS itself [34, 36]. The encrypted DEK is typically stored alongside the encrypted data it was used for [34, 36].
◦
Example: You have a large video file (much bigger than 4KB) you want to encrypt [36]. You ask KMS (using your "MyAppDataKey" KMS key) to generate a data encryption key [36]. KMS gives you two copies of a new, temporary key: one is the readable key, and the other is that same key encrypted by "MyAppDataKey" [36]. You use the readable key to encrypt your large video file, then immediately discard the readable key [36]. You store the encrypted video file and the encrypted temporary key together [34, 36]. When you need to decrypt the video, you give the encrypted temporary key back to KMS, KMS decrypts it using "MyAppDataKey" (which only KMS has), gives you the readable temporary key, you use it to decrypt the video, and then discard the readable temporary key again [34].
•
KMS Key Policies (vs. Identity Policies)
◦
Note: Every KMS key has a key policy, which is a type of resource policy attached to the key [32, 37, 38]. The key policy defines who can use the key and what they can do (e.g., encrypt, decrypt, manage the key) [37, 38]. Unlike many other AWS services, KMS keys, by default, do not automatically trust the AWS account they are in [37, 38]. The key policy must explicitly grant the account (or identities within it) permission to manage or use the key [37-39]. Access to KMS keys is often managed using a combination of a key policy that grants account-level trust and identity policies that grant specific user permissions [38]. Permissions within KMS are very granular [33, 38].
◦
Example: You create a KMS key [35]. By default, not even the root user can use it for encryption or decryption [Based on 44]. You must add a statement to the key policy explicitly allowing your AWS account (identified by its account ID) to perform KMS actions on the key [38, 39]. Then, you can use identity policies on your users to grant them specific permissions (like kms:Encrypt, kms:Decrypt) to use that key [38]. If the key policy doesn't explicitly allow your account, even a user with an identity policy allowing kms:Encrypt won't be able to use the key [38].
•
KMS Role Separation
◦
Note: SSE-KMS in S3 is a primary way KMS enables role separation [40, 41]. Because decrypting SSE-KMS encrypted objects requires permission to use the specific KMS key, you can create an S3 administrator who has full control over the S3 bucket (upload, delete, manage objects) but lacks permissions on the KMS key [41]. This means they can manage the objects but cannot decrypt or view the data within those encrypted objects [41-44].
◦
Example: Phil is an S3 administrator with permissions to manage all objects in a bucket [41]. The sensitive data in the bucket is encrypted using SSE-KMS with a specific KMS key [40, 41]. Phil's identity policy gives him full S3 access (s3:*) but no permissions on the KMS key (kms:* is denied or not granted) [43]. Phil can upload new files, delete files, or change storage classes, but when he tries to download an SSE-KMS encrypted file, S3 attempts to use the KMS key to decrypt it, and KMS denies this request because Phil lacks permission on the key [43]. He gets an "Access Denied" error, achieving role separation – he can manage the container (the bucket) but not see inside the encrypted contents [44, 45].
•
KMS Key Rotation
◦
Note: Key rotation changes the physical backing key material periodically while retaining older material [37]. This helps limit the amount of data encrypted with a single key version [Based on 43]. For AWS managed KMS keys (created automatically by AWS services like S3), rotation is enabled by default and happens approximately once per year, and you cannot disable it [37, 45, 46]. For customer managed KMS keys (created explicitly by the customer), rotation is optional but enabled by default approximately once per year; you can disable it [37, 46, 47]. Data encrypted with older key material versions can still be decrypted by the current version of the KMS key [37].
◦
Example: You create a customer managed KMS key and enable rotation [37, 46]. After about a year, KMS automatically creates new physical key material for this key [37]. Any new data you encrypt will use the new material. However, if you need to decrypt data that was encrypted a year ago using the old material, the KMS key still retains the old material and can decrypt it seamlessly [37]. If it was an AWS managed key created by S3, rotation would happen automatically [37, 46].
•
S3 Encryption (General Concepts - Client-side vs. Server-side)
◦
Note: S3 object encryption primarily refers to encryption at rest (how objects are stored persistently) [48]. Encryption in transit (using HTTPS) is standard for S3 operations [48, 49].
◦
Client-side encryption: The data is encrypted by the client before it is sent to S3 [50]. S3 receives and stores the data in its encrypted form and never sees the plain text [50]. The customer manages the encryption process, keys, and tooling [42, 50].
◦
Server-side encryption: The data is encrypted by the S3 infrastructure after it is received, but before it is stored [50]. S3 receives the data in plain text (within the encrypted HTTPS tunnel) and then encrypts it [50]. S3 handles some or all of the encryption process and key management depending on the specific type [50].
◦
Example: * Client-side: You encrypt a file on your computer using your own software and encryption key. Then, you upload the already encrypted file to S3 [50]. From S3's perspective, it's just storing a scrambled file [50]. * Server-side: You upload a regular, unencrypted file to S3 over HTTPS [50]. Once the file arrives at S3, S3 itself encrypts the file before writing it to disk [50].
•
S3 Server-side Encryption with Customer-Provided Keys (SSE-C)
◦
Note: With SSE-C, the customer provides and manages the encryption key, but S3 performs the actual encryption and decryption operations [51]. When uploading an object, the customer provides the plain text object and the encryption key [49, 51]. S3 encrypts the object with the provided key, takes a one-way hash of the key (to verify the key during decryption), and then discards the key [49]. S3 stores the encrypted object and the key hash [49]. To decrypt, the customer requests the object and provides the same key [49]. S3 verifies the key using the hash, decrypts the object, discards the key, and returns the plain text data [49]. S3 cannot replicate objects encrypted with SSE-C because it doesn't store the key needed to access the plain text [52].
◦
Example: You have a file and an encryption key you generated and manage [51]. When you upload the file to S3, you tell S3 to use SSE-C and give S3 your key just for that operation [49, 51]. S3 encrypts the file using your key, confirms it worked using a hash of the key, and then immediately forgets your key [49]. When you download the file, you tell S3 "use SSE-C" and provide your key again. S3 uses the key to decrypt the file and gives you the original back, again forgetting the key afterwards [49]. You are responsible for keeping track of which key was used for which file [49, 50].
•
S3 Server-side Encryption with S3-Managed Keys (SSE-S3)
◦
Note: With SSE-S3 (using AES 256 encryption [42, 53]), AWS handles both the encryption/decryption process and the key generation/management [53]. S3 uses a root key to encrypt unique keys generated for each object [53]. The customer simply uploads plain text data and tells S3 to use SSE-S3 [53]. S3 manages the entire process, including key rotation, which is not visible or controllable by the customer [53]. It's a common default choice when encryption is needed without specific regulatory key control requirements [42, 53]. However, it does not support role separation, as anyone with full S3 admin permissions can access the decrypted data [40].
◦
Example: You upload a file to S3 and choose SSE-S3 encryption [54]. S3 automatically handles everything: it creates a unique encryption key specifically for this file, encrypts the file with that key, encrypts that unique key with a master key S3 manages, and stores both encrypted items [53]. You don't see or manage any of the keys [53]. When you download the file, S3 automatically uses its keys to decrypt the object for you [53].
•
S3 Server-side Encryption with KMS-Managed Keys (SSE-KMS)
◦
Note: With SSE-KMS, AWS handles the encryption/decryption processes (using data encryption keys generated by KMS for each object [40]) and key management, but the root key (the KMS key) is managed by the KMS service [40]. This KMS key can be an AWS-managed key (created by AWS for S3) or a customer-managed key (created and controlled by the customer) [40, 55]. Using a customer-managed KMS key allows the customer to control key permissions, rotation, and view audit logs (via CloudTrail) [41, 47]. Crucially, SSE-KMS enables role separation: an S3 admin needs separate permissions on the KMS key to decrypt objects, allowing management of the bucket without access to the data [41-44].
◦
Example: You upload a sensitive file to S3 and choose SSE-KMS, selecting a customer-managed KMS key you created [55]. For this object, S3 asks your KMS key to generate a temporary data encryption key [40]. KMS provides S3 with the plain text and encrypted versions of this temporary key [40]. S3 encrypts the object with the plain text key (and discards it), and stores the encrypted object along with the encrypted temporary key (encrypted by your KMS key) [40, 41]. To decrypt, S3 must go back to KMS and ask your KMS key to decrypt the encrypted temporary key [43]. If the user accessing the file doesn't have permissions to use your KMS key for decryption, they cannot access the file, even if they have full S3 permissions [43, 44].
•
S3 Default Bucket Encryption
◦
Note: This is a setting on an S3 bucket that specifies the encryption type to be used by default for any new objects uploaded to the bucket that do not explicitly specify an encryption method in the upload request [42, 56]. It can be set to SSE-S3 (AES 256) or SSE-KMS (using an AWS-managed key or a specific customer-managed key) [47, 56]. The key thing is that the default setting is not a restriction; it does not prevent users from uploading objects with a different encryption type than the default, or even with no encryption, if they specify it in the upload request [44, 56].
◦
Example: You configure a bucket's default encryption to use SSE-S3 [47]. When someone uploads a file to this bucket without specifying any encryption setting, the file will automatically be encrypted with SSE-S3 [47]. However, if they upload another file and explicitly choose SSE-KMS during the upload, that file will be encrypted with SSE-KMS, overriding the bucket default for that specific upload [44, 56].
•
S3 Object Storage Classes
◦
Note: Different storage classes offer trade-offs between cost, performance (access time), and durability/availability [57]. * S3 Standard: Default class. For frequently accessed, non-replaceable data. High durability (11 9s), replicated across >=3 Availability Zones (AZs) [57]. Millisecond access [57]. Billed for storage, data transfer out, and requests [57]. No retrieval fees, minimum duration, or minimum object size [57]. * S3 Standard-Infrequent Access (Standard-IA): For long-lived, infrequently accessed, important/irreplaceable data [57]. Same durability, replication (>=3 AZs), and millisecond access as Standard [57]. Lower storage cost than Standard [57]. Has a retrieval fee (per GB accessed), a 30-day minimum storage duration charge, and a 128 KB minimum object size charge [57]. * S3 One Zone-Infrequent Access (One Zone-IA): For long-lived, infrequently accessed, non-critical or easily replaceable data (e.g., replica copies) [57, 58]. Stored in one AZ only [58]. Lower storage cost than Standard-IA [58]. Has retrieval fees, 30-day minimum duration, and 128 KB minimum object size charges [58]. Higher risk of data loss if that single AZ fails [58]. * S3 Glacier Instant Retrieval: For archival data accessed perhaps once a quarter, where instant retrieval (milliseconds) is still needed [58]. Cheaper storage than Standard-IA, but higher retrieval costs [58]. 90-day minimum storage duration charge [58]. * S3 Glacier Flexible Retrieval (formerly S3 Glacier): For archival data rarely accessed (e.g., yearly), where retrieval time of minutes to hours is acceptable [58, 59]. Very low storage cost [58, 59]. Not immediately available; requires a retrieval job (Expedited: 1-5 mins, Standard: 3-5 hrs, Bulk: 5-12 hrs) which costs money [59]. Cannot be made public [59]. 40 KB minimum billable size, 90-day minimum duration [59]. * S3 Glacier Deep Archive: Cheapest storage class [60]. For archival data accessed very rarely, where retrieval time of hours to days is acceptable [60]. Requires retrieval job (Standard: 12 hrs, Bulk: up to 48 hrs) [60]. Cannot be made public [60]. 40 KB minimum billable size, 180-day minimum duration [60]. Not suited for primary backups due to retrieval time [60]. * S3 Intelligent Tiering: Automatically moves objects between tiers based on access patterns [60]. Contains frequent access (like Standard), infrequent access (like Standard-IA), and optional archive tiers (like Glacier Instant, Glacier Flexible, Deep Archive) [61]. Suitable for data with changing or unknown access patterns [61, 62]. Incurs a small monitoring and automation cost per 1,000 objects instead of retrieval costs for automatic tiering [62]. Instant retrieval from frequent/infrequent/archive instant tiers; archive access/deep archive tiers require retrieval time and specific API calls [61, 62].
◦
Example: * Store frequently accessed website images in S3 Standard [57]. * Store logs you access once a month for compliance in S3 Standard-IA [57]. * Store a redundant copy of video transcodes that can be easily recreated if lost in S3 One Zone-IA [58]. * Store financial records needed for audits quarterly in S3 Glacier Instant Retrieval [58]. * Store backups of old project data only needed for disaster recovery (and you can wait a few hours) in S3 Glacier Flexible Retrieval [59]. * Store long-term legal archives needed maybe once every few years in S3 Glacier Deep Archive [60]. * If you have data where you don't know if it will be accessed frequently or infrequently, use S3 Intelligent Tiering and let S3 move it to the correct tier automatically [61, 62].
•
S3 Life Cycle Configuration
◦
Note: Life cycle rules automate the transition of objects between storage classes (Transition actions) or the deletion of objects/versions (Expiration actions) based on defined criteria, usually time [63]. Rules can apply to whole buckets or filtered groups of objects (by prefix or tags) [63]. They are useful for optimizing storage costs over time [63]. Transitions flow "downhill" (e.g., Standard to Standard-IA, Standard-IA to Glacier, but not Glacier to Standard) [63, 64]. Objects must be in S3 Standard for 30 days before transitioning to Standard-IA or One Zone-IA via a life cycle rule [63]. A single rule transitioning through infrequent access to Glacier tiers requires a 30-day wait in the infrequent access tier; two separate rules can avoid this specific wait [63]. Life cycle rules are not based on access patterns; Intelligent Tiering does that [63]. They can also work on object versions in versioned buckets [63].
◦
Example: You store new customer uploads in S3 Standard for the first 30 days when they are frequently accessed [63]. You set a life cycle rule that automatically transitions these objects to S3 Standard-IA after 30 days, as access becomes less frequent [63]. Then, you might add another rule (or a second rule for immediate effect after the first transition) to move them to S3 Glacier Flexible Retrieval after 90 days total (60 days in Standard-IA), because they are now archival [63, 64]. You could also add an expiration action to delete them after 7 years if required by policy [63].
•
S3 Replication (CRR, SRR)
◦
Note: Replication copies objects from a source bucket to a destination bucket [64]. Cross-Region Replication (CRR) copies between buckets in different AWS regions [64, 65]. Same-Region Replication (SRR) copies between buckets in the same region [64]. Both support replication between buckets in the same account or different accounts [64]. A replication configuration is set on the source bucket and specifies the destination and an IAM role for S3 to assume for the replication process [64]. For cross-account replication, the destination bucket policy must also allow the source account's replication role to write objects [66]. Replication is one-way only [52]. Both source and destination buckets must have versioning enabled [52, 67, 68]. Replication is not retroactive; only objects added after replication is configured are replicated, unless you explicitly choose to replicate existing objects during setup [52, 69].
◦
Replication supports unencrypted objects, SSE-S3 encrypted objects, and SSE-KMS encrypted objects (with extra config/permissions) [52]. It does not support SSE-C encrypted objects (S3 doesn't have the key) [52]. By default, delete markers (object deletions in versioned buckets) are not replicated, but this can be enabled [69, 70]. Replication time control (RTC) provides a 15-minute SLA for replication (99.99% of objects), adding cost [52, 69, 71]. User events are replicated, but system events (like Life Cycle deletions) are not [70]. Objects in Glacier or Glacier Deep Archive storage classes cannot be replicated using this process [70].
◦
Use cases: CRR for disaster recovery (copying data to another region) [65], reducing latency (copying data closer to users) [72], global resilience [72]. SRR for log aggregation (copying logs from multiple buckets to one) [70], synchronizing data between production/test accounts [72], implementing resilience/account isolation within the same region for sovereignty requirements [72].
◦
Example: To create a disaster recovery copy of your important files, you enable CRR from your "source-bucket" in US-East-1 to a "destination-bucket" in US-West-2 [65]. You ensure both buckets have versioning enabled [67, 68]. Any new files you upload to "source-bucket" will automatically be copied to "destination-bucket" by S3 using the configured replication rule and IAM role [64, 73]. Files that existed in "source-bucket" before enabling replication will not be copied unless you configure that option [52, 69].
•
S3 Pre-signed URLs
◦
Note: A pre-signed URL grants temporary, time-limited access to a specific object in a private S3 bucket using the security credentials of the identity that generated the URL [74-76]. This allows users without AWS credentials or identities to access objects [74]. The URL encodes the bucket, object key, expiry date/time, and authentication information [75]. The holder of the URL interacts with S3 as the generating identity [75, 77]. Pre-signed URLs can be used for both downloading (GET) and uploading (PUT) objects [75]. They are useful for granting temporary access without making buckets public or creating AWS identities for every user [74, 78].
◦
Important points: * You can generate a pre-signed URL for an object that the generating identity currently has no access to [77, 79, 80]. The URL will also have no access when used [77, 79, 80]. * When a pre-signed URL is used, its permissions match the current permissions of the generating identity, not the permissions the identity had when the URL was created [77, 79, 80]. If the generating identity loses access to S3 after generating the URL but before the URL expires, the URL will stop working [77, 79]. * It is not recommended to generate pre-signed URLs using credentials obtained by assuming an IAM Role [81, 82]. The temporary credentials from a role may expire before the pre-signed URL, causing the URL to stop working [81, 82]. Use long-term identities like IAM users [81]. * You can generate a pre-signed URL for an object that does not exist [80]. Using this URL will result in a "specified key does not exist" error [80].
◦
Example: You have a private S3 bucket with a report file that you need to share with a partner for only 24 hours [74, 75]. Instead of making the bucket public or creating an AWS account for the partner, you, as an IAM user with permission to read the file, generate a pre-signed URL for that specific report file and set its expiry for 24 hours [75, 76]. You give this URL to the partner. The partner can use the URL to download the file using your permissions, but only for the next 24 hours [75, 76]. After 24 hours, the URL will no longer work [76].
•
S3 Select / Glacier Select
◦
Note: These features allow you to retrieve parts of an object rather than the entire object [83]. You provide a SQL-like statement to the service, and S3/Glacier processes it and returns only the requested data [83]. This reduces the amount of data transferred, resulting in faster performance and lower costs [83]. The filtering is done server-side (within S3/Glacier) before data is sent to the client [83]. Supports formats like CSV and JSON, including compressed versions [83]. Requires explicit use by applications [83].
◦
Example: You have a 1GB log file in S3, formatted as CSV [83]. You only need the log entries from a specific date [83]. Instead of downloading the entire 1GB file and filtering it on your computer, you can use S3 Select and provide a query like SELECT * FROM s3object s WHERE s.date = 'YYYY-MM-DD' [Based on 104]. S3 processes this query, finds the relevant lines in the 1GB file, and only sends those lines to your application [83]. You only pay for the data transferred (the filtered results), not the entire 1GB [83].
•
S3 Event Notifications
◦
Note: This feature allows you to configure an S3 bucket to send notifications when specific events occur within the bucket [84]. Events include object creation (Put, Post, Copy, Multi-part upload completion), object deletion (Delete, Delete Marker creation), object restores (from Glacier), and replication events [84]. Notifications can be delivered to destinations like SNS topics, SQS queues, or Lambda functions [84]. This enables event-driven workflows [84]. Resource policies are needed on the destination services (SQS, Lambda, SNS) to allow the S3 service principal to send notifications [84]. EventBridge is a more modern alternative with broader event support and integrations [84].
◦
Example: You set up an event notification on a bucket where users upload images [84]. You configure it to trigger a Lambda function whenever a new object is created (s3:ObjectCreated:*) [84]. The Lambda function is triggered, processes the newly uploaded image (e.g., resizes it, adds a watermark), and stores the result in another bucket [Based on 105]. This automates image processing whenever a new image is uploaded [84].
•
S3 Access Logs
◦
Note: Access logging provides detailed records of requests made to an S3 source bucket [85]. Logs are delivered as files to a designated target bucket [85]. This is a best-efforts process and changes can take hours to take effect [85]. The S3 log delivery group requires write access to the target bucket, typically granted via an ACL on the target bucket [85]. Log files contain records with attributes like date/time, requestor, operation, status code, etc., space-delimited within records and new-line delimited between records [85]. Useful for security audits, understanding access patterns, and analyzing S3 costs [85]. The customer is responsible for managing the life cycle/deletion of log files in the target bucket [85]. A single target bucket can receive logs from multiple source buckets, using prefixes for separation [85].
◦
Example: To monitor who is accessing the files in your production S3 bucket, you enable access logging on that bucket and configure a separate "logs-bucket" as the target [85]. S3's logging service will periodically deliver log files to "logs-bucket" containing details about each request made to the production bucket [85]. You can then analyze these log files to see which users accessed which files and when [85].
•
S3 Object Lock (General)
◦
Note: Object Lock is a set of features that implement a write-once-read-many (WORM) model, preventing object versions from being overwritten or deleted for a set time or until a legal hold is removed [86]. It must be enabled on new S3 buckets; enabling it on existing buckets requires contacting AWS support [86]. Enabling Object Lock also automatically enables versioning on the bucket, as the lock applies to individual object versions [86]. Once enabled on a bucket, Object Lock cannot be disabled, nor can versioning be suspended [86]. There are two main methods: Retention Periods and Legal Holds, which can be applied at the object version level or set as bucket defaults [86].
◦
Example: To ensure sensitive records cannot be changed or deleted for a specific period, you enable Object Lock when creating the S3 bucket [86]. This prevents anyone, even administrators, from modifying or deleting object versions while the lock is active [86].
•
S3 Object Lock Retention Periods (Compliance Mode)
◦
Note: Compliance mode is the most strict Object Lock mode [87]. When applied to an object version with a specified retention period (in days/years), that object version cannot be deleted or overwritten for the entire duration [87]. Furthermore, the retention period cannot be reduced, and the retention mode cannot be changed for that object version while the lock is active [87]. This applies to all identities, including the account root user [87]. Used for strict regulatory compliance requirements [87].
◦
Example: For critical financial records that must be retained unaltered for 7 years according to regulations, you upload them to a bucket with Object Lock enabled and apply a Compliance mode retention period of 7 years to each object version [87]. For the next 7 years, no one, not even the main AWS account administrator, can delete, change, or reduce the retention period for those specific object versions [87].
•
S3 Object Lock Retention Periods (Governance Mode)
◦
Note: Governance mode is less strict than Compliance mode [87]. When applied with a retention period, the object version still cannot be deleted or changed by default [87]. However, users with the special permission s3:BypassGovernanceRetention can override the governance mode retention and make changes (including deleting the object version or adjusting the lock settings) by including a specific header (x-amz-bypass-governance-retention: true) in their request [87, 88]. The AWS console often includes this header by default if the user has the bypass permission [87]. Useful for preventing accidental deletion, internal governance, or testing settings before using Compliance mode [87].
◦
Example: You use Governance mode with a 1-year retention period for project documents to prevent accidental deletion by most users [87]. Your senior administrators are granted the s3:BypassGovernanceRetention permission [87]. If a project requirement changes, a senior admin can use their bypass permission (e.g., via the console or CLI with the special header) to remove the lock or delete a document version that is no longer needed, even if the 1-year period hasn't passed [87, 88].
•
S3 Object Lock Legal Holds
◦
Note: Legal Holds are a distinct Object Lock feature from Retention Periods [86, 89]. There is no retention period; a legal hold is simply enabled (ON) or disabled (OFF) for an object version [89]. While ON, the object version cannot be deleted or overwritten [89]. A specific permission, s3:PutObjectLegalHold, is required to apply or remove a legal hold [89]. Used to protect specific object versions for legal or project-specific reasons without a fixed end date [89]. Can be used in conjunction with Retention Periods [88].
◦
Example: During a lawsuit, you need to ensure specific emails archived in S3 are preserved indefinitely until the case is resolved [89]. You apply a Legal Hold to the relevant email object versions [89]. This prevents deletion [89]. Years later, when the lawsuit ends, you can remove the Legal Hold (if you have the s3:PutObjectLegalHold permission), at which point the object version is no longer protected by the legal hold and standard permissions/retention periods would apply [89].
